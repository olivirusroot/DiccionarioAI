# Diccionario Ampliado de IA Generativa

## Términos y Definiciones

1. **Algoritmo Generativo**: Sistema computacional diseñado para crear contenido nuevo a partir de patrones aprendidos de datos existentes, en lugar de seguir instrucciones explícitas. Estos algoritmos pueden generar texto, imágenes, música, código y otros tipos de contenido.

2. **Aprendizaje Profundo (Deep Learning)**: Subconjunto del aprendizaje automático que utiliza redes neuronales con múltiples capas para analizar diversos factores de datos con una estructura similar a la del cerebro humano.

3. **Arquitectura Transformer**: Diseño de red neuronal introducido por Google en 2017 que revolucionó el procesamiento del lenguaje natural. Utiliza mecanismos de atención para procesar secuencias de datos en paralelo.

4. **Atención (Attention Mechanism)**: Componente clave de las arquitecturas transformer que permite al modelo enfocarse en diferentes partes de los datos de entrada al generar cada elemento de salida.

5. **Autoregresión**: Técnica utilizada en modelos generativos donde la salida se genera secuencialmente, con cada nuevo elemento condicionado por los elementos previamente generados.

6. **BERT (Bidirectional Encoder Representations from Transformers)**: Modelo de lenguaje desarrollado por Google que procesa texto de manera bidireccional, considerando el contexto tanto anterior como posterior de cada palabra.

7. **Cadena de Pensamientos (Chain of Thought)**: Técnica de ingeniería de instrucciones que fomenta que un modelo de lenguaje extenso explique su razonamiento paso a paso, mejorando la precisión en tareas complejas.

8. **ChatGPT**: Modelo conversacional desarrollado por OpenAI basado en la arquitectura GPT, optimizado específicamente para mantener diálogos coherentes y útiles con humanos.

9. **Claude**: Asistente de IA conversacional desarrollado por Anthropic, diseñado con un enfoque en seguridad, honestidad y utilidad mediante la técnica de "IA Constitucional".

10. **Codificador (Encoder)**: Componente de una arquitectura de red neuronal que transforma datos de entrada en representaciones vectoriales de alta dimensión (embeddings).

11. **Decodificador (Decoder)**: Componente de una arquitectura de red neuronal que transforma representaciones vectoriales internas en datos de salida como texto generado.

12. **Difusión (Diffusion Models)**: Clase de modelos generativos que funcionan añadiendo ruido gradualmente a los datos y luego aprendiendo a revertir este proceso, utilizados principalmente para generación de imágenes.

13. **DALL-E**: Sistema de IA desarrollado por OpenAI que genera imágenes a partir de descripciones textuales, utilizando una variante de GPT-3 adaptada para comprender relaciones entre conceptos visuales.

14. **Embeddings**: Representaciones vectoriales de datos (palabras, frases, imágenes) en un espacio multidimensional donde la proximidad indica similitud semántica.

15. **Evaluación Automática**: Proceso que utiliza software para juzgar la calidad del resultado de un modelo de IA generativa, comparándolo con respuestas ideales o utilizando métricas predefinidas.

16. **Evaluación del Evaluador Automático**: Mecanismo híbrido para juzgar la calidad del resultado de un modelo de IA generativa que combina la evaluación humana con la evaluación automática.

17. **Fine-tuning (Ajuste Fino)**: Proceso de adaptar un modelo preentrenado a una tarea específica mediante entrenamiento adicional con un conjunto de datos más pequeño y especializado.

18. **Función de Pérdida (Loss Function)**: Medida matemática que cuantifica la diferencia entre las predicciones de un modelo y los valores reales deseados durante el entrenamiento.

19. **GANs (Redes Generativas Adversarias)**: Arquitectura que enfrenta dos redes neuronales: un generador que crea contenido y un discriminador que evalúa su autenticidad, mejorando iterativamente.

20. **Gemini**: Modelo multimodal desarrollado por Google DeepMind, diseñado para comprender y generar contenido en múltiples formatos (texto, imágenes, audio, video).

21. **Generación Condicionada**: Técnica donde el proceso generativo se guía mediante condiciones o restricciones específicas proporcionadas como entrada, como descripciones textuales para imágenes.

22. **GenAI (IA Generativa)**: Subcampo de la inteligencia artificial que se enfoca en crear contenido nuevo a partir de datos existentes, pudiendo producir texto, imágenes, música y más de manera creativa y autónoma.

23. **GPT (Generative Pre-trained Transformer)**: Familia de modelos de lenguaje desarrollados por OpenAI que utilizan la arquitectura transformer con un enfoque autoregresivo para generar texto.

24. **Gradiente Descendente**: Algoritmo de optimización utilizado para entrenar redes neuronales, que ajusta iterativamente los parámetros del modelo en la dirección que reduce la función de pérdida.

25. **Hallucination (Alucinación)**: Fenómeno donde un modelo generativo produce contenido que parece plausible pero es factualmente incorrecto o inventado.

26. **Hiperparámetros**: Variables configurables que determinan la estructura y el proceso de entrenamiento de un modelo, como la tasa de aprendizaje o el tamaño de lote.

27. **In-context Learning (Aprendizaje en Contexto)**: Capacidad de los modelos de lenguaje grandes para adaptar su comportamiento basándose en ejemplos proporcionados dentro del prompt, sin actualizar sus parámetros.

28. **Inferencia**: Proceso de utilizar un modelo entrenado para generar predicciones o contenido nuevo a partir de entradas específicas.

29. **Instrucción con Varios Ejemplos (Few-shot Learning)**: Técnica donde se proporciona al modelo algunos ejemplos dentro del prompt para guiar su comportamiento en una tarea específica.

30. **Instrucciones Directas (Zero-shot Learning)**: Técnica donde se le pide al modelo realizar una tarea sin proporcionarle ejemplos previos, confiando en su conocimiento general.

31. **Latent Space (Espacio Latente)**: Representación comprimida y abstracta de datos dentro de un modelo generativo, donde cada punto corresponde a una posible salida.

32. **LangChain**: Framework para desarrollar aplicaciones potenciadas por modelos de lenguaje, facilitando la conexión de LLMs con otras fuentes de datos y aplicaciones.

33. **LLM (Large Language Model)**: Modelo de lenguaje con miles de millones o billones de parámetros, entrenado en vastos corpus de texto para adquirir capacidades lingüísticas y conocimientos generales.

34. **LLM Agents (Agentes LLM)**: Sistemas que utilizan modelos de lenguaje para interactuar con herramientas externas, tomar decisiones y ejecutar acciones en entornos digitales o físicos.

35. **Manus**: Agente de IA avanzado diseñado para realizar tareas complejas de forma autónoma, utilizando herramientas y ejecutando flujos de trabajo completos con razonamiento sofisticado.

36. **Memoria de Contexto**: Capacidad de un modelo para mantener y utilizar información de interacciones previas dentro de una conversación o sesión.

37. **Modelo Base**: Versión inicial de un modelo de IA entrenado en un corpus grande y diverso, diseñado para capturar conocimientos generales antes de cualquier especialización.

38. **Modelo Multimodal**: Sistema de IA capaz de procesar y generar múltiples tipos de datos (texto, imágenes, audio, video) de manera integrada.

39. **MoE (Mixture of Experts)**: Arquitectura que divide un modelo grande en subcomponentes especializados ("expertos") que se activan selectivamente según la entrada.

40. **Multimodal Chain-of-Thought**: Extensión del razonamiento de cadena de pensamiento a contextos que involucran múltiples modalidades de información (texto, imágenes, etc.).

41. **One-shot Learning**: Capacidad de un modelo para aprender a realizar una tarea nueva a partir de un solo ejemplo, en contraste con el aprendizaje tradicional que requiere muchos ejemplos.

42. **Overfitting (Sobreajuste)**: Fenómeno donde un modelo aprende patrones específicos de los datos de entrenamiento tan detalladamente que pierde capacidad de generalización a datos nuevos.

43. **Parámetros**: Variables ajustables dentro de un modelo de IA que se modifican durante el entrenamiento para capturar patrones en los datos.

44. **Perplexity (Perplejidad)**: Medida matemática que evalúa qué tan bien un modelo de lenguaje predice una muestra de texto, con valores más bajos indicando mejor rendimiento.

45. **Prompt Engineering**: Práctica de diseñar y optimizar instrucciones textuales para modelos generativos con el fin de obtener resultados específicos y de alta calidad.

46. **RAG (Retrieval-Augmented Generation)**: Técnica que combina la generación de contenido con la recuperación de información de fuentes externas para mejorar la precisión y reducir alucinaciones.

47. **Red Neuronal**: Sistema computacional inspirado en la estructura del cerebro humano, compuesto por nodos interconectados (neuronas artificiales) organizados en capas.

48. **RLHF (Reinforcement Learning from Human Feedback)**: Técnica que utiliza evaluaciones humanas para refinar el comportamiento de modelos generativos, alineando sus salidas con las expectativas humanas.

49. **Sampling (Muestreo)**: Proceso de seleccionar la siguiente palabra o token durante la generación de texto, utilizando diferentes estrategias como temperatura o top-k.

50. **Stable Diffusion**: Modelo de difusión latente de código abierto para generación de imágenes a partir de descripciones textuales, democratizando el acceso a esta tecnología.

51. **Temperatura**: Hiperparámetro que controla la aleatoriedad en la generación de contenido, con valores más altos produciendo resultados más diversos pero potencialmente menos coherentes.

52. **Tokenización**: Proceso de dividir texto en unidades más pequeñas (tokens) que el modelo puede procesar, pudiendo ser palabras, partes de palabras o caracteres.

53. **Transfer Learning**: Técnica donde un modelo entrenado para una tarea se reutiliza como punto de partida para una tarea diferente pero relacionada.

54. **Underfitting (Subajuste)**: Situación donde un modelo es demasiado simple para capturar la complejidad de los datos, resultando en un rendimiento deficiente.

55. **VAE (Variational Autoencoder)**: Tipo de modelo generativo que aprende a codificar datos en una distribución probabilística en un espacio latente y luego decodificar muestras.

56. **Ventana de Contexto**: Cantidad máxima de tokens que un modelo puede procesar simultáneamente, limitando la cantidad de información que puede considerar al generar respuestas.

57. **Weight Decay (Decaimiento de Pesos)**: Técnica de regularización que penaliza valores grandes en los parámetros del modelo durante el entrenamiento, ayudando a prevenir el sobreajuste.

58. **Zero-shot Learning**: Capacidad de un modelo para realizar tareas para las que no ha sido específicamente entrenado, basándose únicamente en instrucciones textuales.

59. **A2A (Agent-to-Agent)**: Paradigma de comunicación donde múltiples agentes de IA interactúan entre sí para resolver problemas complejos o colaborar en tareas.

60. **Destilación**: Proceso de reducir el tamaño de un modelo (profesor) en un modelo más pequeño (estudiante) que emula las predicciones del original de manera más eficiente.

61. **Facticidad**: Propiedad que describe un modelo cuyo resultado se basa en la realidad y hechos verificables, en contraste con la creatividad pura.

62. **Gemas (Gems)**: En el contexto de Google, modelos de lenguaje especializados diseñados para tareas específicas dentro del ecosistema Gemini.

63. **Incorporación de Lenguaje Contextualizado**: Tipo de embedding que comprende palabras y frases considerando su contexto específico, captando matices semánticos complejos.

64. **Jailbreaking**: Técnicas utilizadas para eludir las salvaguardas éticas y restricciones de seguridad implementadas en modelos de IA generativa.

65. **NVIDIA Tensor Cores**: Unidades de procesamiento especializadas en GPUs de NVIDIA diseñadas específicamente para acelerar operaciones de álgebra matricial utilizadas en redes neuronales.

66. **Chatear**: Contenido de un diálogo de ida y vuelta con un sistema de IA, donde la interacción anterior se convierte en contexto para las partes posteriores de la conversación.

67. **Evals**: Abreviatura de evaluaciones, especialmente en el contexto de medir el rendimiento y la calidad de los modelos de lenguaje grandes.

68. **Midjourney**: Herramienta de IA generativa especializada en crear imágenes artísticas de alta calidad a partir de descripciones textuales.

69. **Anthropic**: Empresa de investigación en IA que desarrolló Claude, enfocada en la seguridad y alineación de sistemas de IA avanzados.

70. **Bard**: Asistente de IA conversacional desarrollado por Google, predecesor de Gemini, diseñado para interactuar con usuarios y proporcionar información.

71. **Copilot**: Asistente de IA desarrollado por Microsoft y GitHub que ayuda a los programadores a escribir código sugiriendo líneas o funciones completas.

72. **Hugging Face**: Plataforma que proporciona herramientas, modelos y datasets para aplicaciones de IA, especialmente en procesamiento de lenguaje natural.

73. **Whisper**: Modelo de reconocimiento de voz desarrollado por OpenAI que puede transcribir y traducir audio en múltiples idiomas con alta precisión.

74. **Sora**: Modelo de IA generativa de OpenAI capaz de crear videos realistas a partir de descripciones textuales, representando un avance en la generación de contenido visual dinámico.

75. **Llama**: Familia de modelos de lenguaje grandes de código abierto desarrollados por Meta (anteriormente Facebook), diseñados para ser más accesibles para investigadores y desarrolladores.

76. **Mistral**: Modelo de lenguaje de código abierto que ofrece capacidades avanzadas con requisitos computacionales más eficientes que otros LLMs de tamaño similar.

77. **Imagen Latente**: Representación comprimida de una imagen en el espacio latente de un modelo generativo, que captura características esenciales de forma matemática.

78. **Alineación de IA**: Proceso de asegurar que los sistemas de IA actúen de acuerdo con las intenciones y valores humanos, minimizando comportamientos no deseados.

79. **Sesgo Algorítmico**: Tendencia sistemática de un modelo de IA a producir resultados que favorecen o perjudican injustamente a ciertos grupos, reflejando sesgos presentes en los datos de entrenamiento.

80. **Interpretabilidad**: Grado en que se puede entender y explicar el funcionamiento interno y las decisiones de un modelo de IA, crucial para aplicaciones críticas y confianza en los sistemas.
